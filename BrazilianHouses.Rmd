---
title: 'Final Project: a look into Brazilian Houses Market'
output:
  pdf_document: 
  geomwtry: margin:=1in
  html_document: default
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

#### *OPPA Group: Andrea Contino, Pierpaolo Ceccarelli, Piergiorgio Zannella & Omar Regragui*

## 0. Understanding the dataset

In the first part of the project we will investigate some interesting
trends and insights on the house rent market in Brazil, and finding how
they impact the rent cost. Here we'll get a abroad idea of what the
dataset looks like and check any irregularities in the data.

**Important Disclaimer: We understand the importance of providing clear
and smart code, however we prioritized the importance of our findings
due to report size restriction. To check all our elaborations and
techniques tried, and how we implemented the models please refer 
to the complete Rscript code (the code that we only tested is at the end of the Rscript)**

```{r, include = FALSE}
# Function to check and install packages
check_and_install <- function(package){
  if (!package %in% installed.packages()) {
    install.packages(package)
  }
  library(package, character.only = TRUE)
}

# Using the function to install and load the packages
packages <- c("dplyr", "ggplot2", "cowplot", "corrplot", "GGally", 
              "gridExtra", "glmnet", "MASS", "caret", "randomForest", 
              "mgcv", "purrr","mice", "cowplot","tidyverse", "factoextra", 
              "cluster", "patchwork", "RColorBrewer")
lapply(packages, check_and_install)
```

## 1. Data cleaning and preparation

```{r, echo = FALSE}
# Importing the dataset
raw_data <- read.csv("BrazHousesRent.csv")
```

```{r, include = FALSE}
# Floor variable is a character but it should be a number so we convert it
raw_data$floor <- as.numeric(raw_data$floor)

# Looking for missing values
sapply(raw_data, function(x) sum(is.na(x))) # floor variables contains nulls

# Treating null in the floor variable
data <- raw_data

# Removing duplicate values if any
data <- data[!duplicated(data),]  #dataframe with no duplicates
```

Some duplicates were found and nulls in the **`floor`** column. We
decided to remove the duplicates and treat the nulls in the floor
variable with the MICE package's PMM method which stands for
predictive mean matching. This method is basically a regression model
which uses the other variables as predictors to fill the missing value
row, and impute the missing value of the row by using the "similar"
values as predictors.

```{r,include = FALSE}
# Creating the imputation object and using a simple pmm to fill floor's nulls
set.seed(123)
imp_obj <- mice(data, method = "pmm")
imputed <- complete(imp_obj)[,"floor"]

# Replacing the null values with the obtained values for the floor variable
data$floor[is.na(data$floor)] <- imputed[is.na(data$floor)]

# Factoring the other categorical columns
data$city <- as.factor(data$city)
data$animal <- as.factor(data$animal)
data$furniture <- as.factor(data$furniture)

# Renaming columns with clearer names
data <- data %>% rename(
  hoa = hoa..R.., rent = rent.amount..R.., 
  proptax = property.tax..R.., fireins = fire.insurance..R..
  )
```

***A summary of the final cleaned data.***

```{r, echo = FALSE, fig.width=2, fig.height=2, dev.args=list(pointsize=5)}
# Cleaned dataframe summary
summary(data)
```

\nopagebreak

## 2. EDA Analysis

After imputing the data we can now explore more in detail the
distribution of each feature to identify eventual outliers and
irregularities. From the summary, one can immediately see that there are
some outliers in the features the `area`, the `hoa`, the `fire insurance` and the `property taxes` that have some very suspicious maximum, suggesting
that some houses may be completely irrelevant for our analysis. We'll
see by looking at a boxplot and an histogram density plot for each
interesting numerical feature.

```{r, echo = FALSE, tidy = TRUE, fig.width = 8, fig.height = 3, fig.align = 'center'}
# Boxplot and histograms with density lines for interesting numerical columns
num_cols <- sapply(data, is.numeric)
cat_cols <- sapply(data, is.factor)
numcols1 <- c("area", "hoa", "rent", "proptax", "fireins")

# Storing the plots
p_boxplot <- list()
p_boxplot1 <- list()
p_histogram <- list()
p_histogram1 <- list()

# Creating the plots
for (col in names(data[numcols1])) {
  # Boxplot
  p_boxplot[[col]] <- ggplot(data, aes(y = !!sym(col))) +
    geom_boxplot(fill = "#ADD8E6", alpha = 0.5, outlier.color = "#8B0000",
    outlier.shape = 1) +labs(title = paste0(col," boxplot"), x = "") + 
    theme_bw() +
    theme(axis.text.x = element_text(size = 5), 
          axis.text.y = element_text(size = 5))
  
  # Histogram
  p_histogram[[col]] <- ggplot(data, aes(x = !!sym(col))) +
    geom_histogram(fill = "#ADD8E6", alpha = 0.5) +
    geom_freqpoly(color = "#ADD8E6", linewidth = 0.05) +
    labs(title = paste0(col," hist"), y = "", x = "") +
    theme_bw() +
    theme(axis.text.x = element_text( size = 5),  
          axis.text.y = element_text(size = 5))
  
  # Boxplot with log scaled data
  p_boxplot1[[col]] <- ggplot(log(data[,numcols1]), aes(y = !!sym(col))) +
    geom_boxplot(fill = "#ADD8E6", alpha = 0.5, outlier.color = "#8B0000",
    outlier.shape = 1) + labs(title = paste0("log Boxplot ", col), 
    x = "") +theme_bw() +
    theme(axis.text.x = element_text(size = 5),  
          axis.text.y = element_text(size = 5))
  
  # Histogram with scaled data
  p_histogram1[[col]] <- ggplot(log(data[,numcols1]), aes(x = !!sym(col))) +
    geom_histogram(fill = "#ADD8E6", alpha = 0.5) +
    geom_freqpoly(color = "#ADD8E6", linewidth = 0.05) +
    labs(title = paste0("log hist ", col), y = "", x = "") +
    theme_bw() +
    theme(axis.text.x = element_text(size = 5),  
          axis.text.y = element_text(size = 5))
  
  # Extract ggplot objects from lists
  plot1 <- p_boxplot[[col]]
  plot2 <- p_histogram[[col]]
  plot3 <- p_boxplot1[[col]]
  plot4 <- p_histogram1[[col]]
}

# Extract "area" ggplot objects from lists
plot1 <- p_boxplot[["area"]]
plot2 <- p_histogram[["area"]]
plot3 <- p_boxplot1[["area"]]
plot4 <- p_histogram1[["area"]]

# Use the extracted ggplot objects
print(plot_grid(plot1, plot2, plot3, plot4, ncol = 4))
```

***Above, you can see just a snapshot of a significant skewed
distributions (area).***

As suspected most variables present some outliers. All the distributions
are visibly right skewed, with most values being in the interquartile
range and some houses with remarkably high features. Although it is true
that we might log scale to make the distributions more "normal", that
would underestimate the magnitude of the outliers. We decided to get rid
of the *outliers* as they might destabilize our results and the accuracy
of our models. Obviously, we considered only numerical variables that
make that didn't make sense cost wise and not the ones that have many
floors or rooms. In total, ***268 houses were disregarded***.

```{r,include = FALSE}
# Columns to consider for outlier detection
cols <- c("area", "hoa","rent", "proptax")
data_out <- data[, cols]

# Using z-scores method
z_scores <- apply(data_out, 2, 
            function(x) abs((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)))

# Identify rows with at least one z-score greater than 3 (considered outliers)
outlier_rows <- row.names(data_out)[apply(z_scores, 1, function(x) any(x > 3))]

# Print the number of outliers detected
cat("Number of outliers detected:", length(outlier_rows), "\n")

# Print which was the most common rent value among the outliers
cat("Most common rent value among the outliers:", 
    names(sort(table(data[rownames(data) %in% outlier_rows, "rent"]), 
               decreasing = TRUE)[1]), "\n")

# Print how many times the most common rent value appeared among the outliers
cat("Number of times the most common rent value appeared among the outliers:", 
    max(table(data[rownames(data) %in% outlier_rows, "rent"]), "\n"))

# Creating the final dataframe without outliers
final_data <- data[!(rownames(data) %in% outlier_rows), ]
```

```{r, echo = FALSE, fig.width = 10, fig.height = 5, fig.align = 'center'}
par(mfrow=c(2,2))
options(repr.plot.width=12, repr.plot.height=8)
boxplot(data$rent, col = "#ADD8E6", horizontal = T, 
        main = "Rent - Before Removing Outliers")
qqnorm(data$rent)

boxplot(final_data$rent, col = "#98FB98", horizontal = T,
        main = "Rent - After Removing Outliers")
qqnorm(final_data$rent)
```

\nopagebreak

For the outliers removal we decided to apply the *z-scores method*,
setting the threshold to 3. Interesting things came up doing so, for
instance, most of the houses removed had the same rent price of 15000,
so it is not difficult to notice that it may be a trivial data entry error.
Also, some houses were incredibly big but had the same rooms of those
which were way smaller. It is true that the **parking spaces** area
could be included in the total **`area`**, but it's probably not worth
to consider these likely wrong typed data.

By looking at the Q-Q plot it also came up that it is very unlikely that
some of them were actually part of the dataset. Still some extremes are
present in the new data but as already mentioned, it is very common to
have house prices following a right skewed distribution in our opinion.
Most of high ended houses are located in Sao Paulo, where most of the
luxurious houses are at.

Before, we tested the IQR method by setting 1.5 as step for outliers in
the lower quartile and a more relaxed 2 multiplier for extreme values.
The approach with 1.5 as multiplier was too strict, deleting legitimate
houses. The amount of deleted data was considerable ($\approx 33$%), and
it is highly unlikely that this amount of data contain all real
outliers. Indeed, we assume that is completely normal to have very
expensive and fancy houses in specific areas. Once again most of the
detected "false outliers" were located at Sao Paulo, where houses' rents
are generally higher.

### Categorical variables inspection

Now we'll check if there's any correlation between the categorical
variables the rent price.

```{r, echo = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center', results='hide'}
# Boxplots for rental prices by category with red outliers
newcol <- c("furniture", "city","animal")
plots <- list()

for (col in names(final_data[newcol])) {
  p_box <- ggplot(final_data, aes(x = !!sym(col), 
                                  y = rent, fill = factor(.data[[col]]))) +
    geom_boxplot(outlier.shape = 1,outlier.size = 2,outlier.color = "#8B0000") +
    labs(x = col, y = "Rent Amount") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 55, size = 8, hjust = 1),
                       legend.text = element_text(size = 8), 
                       legend.title = element_text(size = 10))
  
  p_hist <- ggplot(final_data, aes(x = !!sym(col))) +
    geom_bar(fill = "#F46D43", alpha = 0.75) +
    labs(title = "Category Frequency", y = "Frequency", x = "Category") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 55, size = 8, hjust = 1),
          legend.text = element_text(size = 8), 
          legend.title = element_text(size = 10))
  
  plots[[col]] <- arrangeGrob(p_box, p_hist, nrow = 2)
}

# Arranging and displaying the plots
final_plot <- grid.arrange(grobs = plots, ncol = 3)
print(final_plot)
```

Houses in Sao Paulo cost more than in other places as the interquartile
and the median show. Also, the majority of the most expensive are
located in this city.

Animal allowance (**`animal`**) does not make a huge difference in rent
prices, furniture obviously makes some difference. The lower quartile of
the furnished home rental price corresponds to the median of not
furnished ones, indicating a relevant difference in rental prices.

\nopagebreak

## 3. Investigating the Correlation among variables

We must assess the relevance of our predictors. We can check if there's
any linear dependence among them by looking at the correlation matrix.

```{r, echo = FALSE, fig.width = 5, fig.height = 5, fig.align = 'center'}
# Creating a correlation matrix
  
# Correlation matrix with numerical variables
corr_matrix <- cor(select_if(final_data, is.numeric))

# Plotting the heatmap
layout(matrix(c(1,1), nrow = 2, ncol = 2), widths = 5, heights = 5)
corr <- corrplot(corr_matrix, method ="color", order = "hclust",
            addCoef.col = "#1a1a1a", tl.cex = 0.8, tl.col = "#1a1a1a",
            col = colorRampPalette(c("#313695", "#4575B4", "#74ADD1", "#ABD9E9", 
                                     "#E0F3F8", "#FFFFBF", "#FEE090", "#FDAE61", 
                                     "#F46D43", "#D73027", "#A50026"))(200))
```

### Conclusions on our correlation heatmap

Looking at the correlation heatmap, it is easy to see why rooms, area
and the rent amounts are correlated. Trivially, one would want to spend
more if there are more rooms and space in a house and viceversa.
On the other hand, it is remarkable the correlation of **`rent`** with
the fire insurance **`fireins`**. It could make sense that the price of
the insurance is correlated with the rent amount, because is how usually works
also for car insurance.There is no apparent risk of multicollinearity among predictors as the only two highly correlated variables are the fire insurance and our target.

### Fitting with one predictor

Since the end goal is to predict the rent prices, it would be
interesting to see the single relationships between predictors and
target (**`rent`**). We will show the variables having the top 4 highest
correlation with rent because the other coefficients are not that
significant (under 0.4). We chose a 95% CI to provide a practical level of 
confidence that allows us to make reasonable decisions while still accounting 
for statistical uncertainty. The fitted regression lines below show the
linear relationship between each chosen predictor and the target.

\nopagebreak

```{r, echo = FALSE, fig.width = 10, fig.height = 8, fig.align = 'center'}
# Find the top 4 correlated variables with rent
top_correlated_vars <- sort(corr_matrix["rent",], decreasing = T)[2:5]
top_correlated_vars_list <- names(top_correlated_vars)
target <- "rent"

# Fitting a simple regression (conf.int. at 95% shown)
p_list <- list() 
for (i in 1:length(top_correlated_vars_list)) {
  p <- ggplot(final_data, aes(x = !!sym(top_correlated_vars_list[i]), 
                              y = !!sym(target))) +
    geom_point(cex = 3, pch = 1, stroke = 2, color="#FFA07A") +
    geom_smooth(method = "lm", color = "#8B0000", lwd = 1, formula = "y~x", 
         se = TRUE, level = 0.95, linewidth = 1) + theme_light(base_size = 16) +
    ggtitle(paste("Scatter plot of", top_correlated_vars_list[i], "vs",target))
  
  p_list[[i]] <- p
}

# Showing plots
grid.arrange(grobs = p_list, ncol = 2)
```

### The importance of fire insurance.

Surprisingly, the scatterplots show that even taking the **`fireins`** as
a single predictor could be a decent solution (using a very simple
linear regression can be seen a clear positive correlation). However
we must investigate other relevant information that may come useful for
our regression. Area also seems to have a positive relation with the
rental prices as well as the property taxes. We will see if there is a
risk of high variability in our models, since the bias should be quite
low.

## Our Objective

Our main objective is to get a comprehensive view of rental prices,
pointing out the major differences by their features.

Doing so we can obtain precious insights on the rental prices that, for
instance, can help buyers chose which house has a fair rent compared to
others with similar features, through our rent prediction.

\nopagebreak

## 4. Testing how using different variables as predictors effects the predictions

### AIC Stepwise methods

Including **`fireins`** dramatically improves the model. To test further
this assumption made through the correlation matrix and the simple
regression model that we test before on the singular predictor, we will
show how this effects the AIC stepwise methods in different ways.

The positive correlation to the target is almost 1, making it the
perfect feature, however evaluating the models without this feature
could be beneficial to confirm the general goodness of the models.

Since AIC and BIC perform roughly the same, we will show the AIC as we don't 
have so many predictors to prefer a more penalized and strict approach.

The AIC provides a good way to balance between complexity and
generalization to new data however they are less reliable, since
variable selection could introduce some bias towards specific features,
for instance, it may give high importance to **`fireins`** and disregard
features which are actually influential because it wants to reduce the
risk of overfitting and model complexity.

***Please Note that you can find the chunk code of the models in the
Rscript. The chunk was very large and hence we did not include it in the
report. Nonetheless, the 4 AIC models are listed below***:

1)  In the first model we test just with the top 2 correlated predictors
    **`fireins`** and **`area`** to understand how it would react to
    them compared to the models with all the predictors

    ``` r
    lm_aic1 <- stepAIC(lm(rent ~ fireins + area, data = train_data), 
                   direction = "both", trace = FALSE)
    ```

2)  Then we test a model without **`fireins`**

    ``` r
    lm_aic2 <- stepAIC(lm(rent ~ hoa + proptax + area + rooms + city + 
                        bathroom + floor + parking.spaces + furniture + animal, 
                      data = train_data), direction = "both", trace = FALSE)
    ```

3)  Here we test the model with all the predictors

    ``` r
    lm_aic3 <- stepAIC(lm(rent ~ ., data = train_data), 
                   direction = "both", trace = FALSE)
    ```

4)  In the end we improved the model with some **feature engineering** to
    reduce predictors (combining correlated and disregarding less
    relevant)

    ``` r
    lm_aic4 <- stepAIC(lm(rent ~ hoa*proptax + area*rooms + city + bathroom + 
                        parking.spaces + fireins + furniture, 
                      data = train_data), direction = "both", trace = FALSE)
    ```

Now let's see how they performed:

```{r, echo = FALSE, fig.height= 5, fig.width= 10}
set.seed(123) # For reproducibility

# Split the data into training and valid(test) sets (80-20 as usual)
train_indices <- sample(1:nrow(final_data), 0.8*nrow(final_data))
train_data <- final_data[train_indices, ]
test_data <- final_data[-train_indices, ]

# Setting up for the models
predictors <- setdiff(names(final_data),c("rent"))
train_x <- train_data[,predictors]
train_y <- train_data$rent
test_x <- test_data[,predictors]
test_y <- test_data$rent

# Creating a dataframe to store all models performance indicators
performance_df <- data.frame(Model = character(), RMSE = numeric(), 
                                 R2 = numeric(), stringsAsFactors = FALSE)
colnames(performance_df) <- c("Model", "RMSE", "R2")

# Creating a dataframe to store the AIC model performance indicators
AIC_performance_df <- data.frame(Model = character(), RMSE = numeric(), 
                             R2 = numeric(), stringsAsFactors = FALSE)

#AIC criterion

# With the top 2 correlated variables "fireins" and "area"
lm_aic1 <- stepAIC(lm(rent ~ fireins + area, data = train_data), 
                   direction = "both", trace = FALSE)

# Without the variable "fireins"
lm_aic2 <- stepAIC(lm(rent ~ hoa + proptax + area + rooms + city + bathroom + 
                        floor + parking.spaces + furniture + animal, 
                      data = train_data), direction = "both", trace = FALSE)

# With all the variables
lm_aic3 <- stepAIC(lm(rent ~ ., data = train_data), 
                   direction = "both", trace = FALSE)

# With some feauture engineering (multiplying closely related variables)
lm_aic4 <- stepAIC(lm(rent ~ hoa*proptax + area*rooms + city + bathroom + 
                        parking.spaces + fireins + furniture, 
                      data = train_data), direction = "both", trace = FALSE)

# Function to calculate R2
calculate_R2 <- function(y_true, y_pred) {
  y_mean <- mean(y_true)
  ss_total <- sum((y_true - y_mean)^2)
  ss_residual <- sum((y_true - y_pred)^2)
  R2 <- 1 - (ss_residual / ss_total)
  return(R2)
}

# Top 2 Model
predictions_aic1 <- predict(lm_aic1, newdata = test_x)
rmse_aic1 <- sqrt(mean((predictions_aic1 - test_y)^2))
R2_aic1 <- calculate_R2(test_y, predictions_aic1)
AIC_performance_df <- rbind(AIC_performance_df, 
                        c("Top 2 AIC Model", rmse_aic1, R2_aic1))

# No fire insurence Model
predictions_aic2 <- predict(lm_aic2, newdata = test_x)
rmse_aic2 <- sqrt(mean((predictions_aic2 - test_y)^2))
R2_aic2 <- calculate_R2(test_y, predictions_aic2)
AIC_performance_df <- rbind(AIC_performance_df, 
                        c("No fireins AIC Model", rmse_aic2, R2_aic2))

# Complete Model
predictions_aic3 <- predict(lm_aic3, newdata = test_x)
rmse_aic3 <- sqrt(mean((predictions_aic3 - test_y)^2))
R2_aic3 <- calculate_R2(test_y, predictions_aic3)
AIC_performance_df <- rbind(AIC_performance_df, 
                        c("Complete AIC Model", rmse_aic3, R2_aic3))

# Feature Engineered Model
predictions_aic4 <- predict(lm_aic4, newdata = test_x)
rmse_aic4 <- sqrt(mean((predictions_aic4 - test_y)^2))
R2_aic4 <- calculate_R2(test_y, predictions_aic4)
AIC_performance_df <- rbind(AIC_performance_df, 
                        c("FE AIC Model", rmse_aic4, R2_aic4))
performance_df <- rbind(performance_df, 
                      c("LM AIC Model", rmse_aic4, R2_aic4))

# Showing the performances of the models
colnames(AIC_performance_df) <- c("Model", "RMSE", "R2")
AIC_performance_df 

AIC_performance_df$RMSE <- as.numeric(AIC_performance_df$RMSE)
AIC_performance_df$R2 <- as.numeric(AIC_performance_df$R2)

```

```{r,include = FALSE}
# Plot for RMSE
p1 <- ggplot(AIC_performance_df, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(RMSE,2)), vjust = -0.5, size = 3) +
  labs(title = "RMSE Model Performances", y = "RMSE", x = "Model") +
  theme_light(base_size = 13) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill=FALSE)

# Plot for R2
p2 <- ggplot(AIC_performance_df, aes(x = Model, y = R2, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(R2,5)), vjust = -0.5, size = 3) +
  labs(title = "R-Squared Model Performances", y = "R2", x = "Model") +
  theme_light(base_size = 13) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Arranging the plots in a single grid
grid.arrange(p1, p2, ncol = 2)

# Checking if residuals are normally distributed

# Creating the grid for the visualizzation
par(mfrow = c(2, 2))

set.seed(123) 

# Top 2 Model
residuals_aic1 <- residuals(lm_aic1)
hist(residuals_aic1, breaks = 20, 
     main = "Residuals - Top 2 Model", xlab = "Residuals")


# No fire insurence Model
residuals_aic2 <- residuals(lm_aic2)
hist(residuals_aic2, breaks = 20, 
     main = "Residuals -  No fire insurence Model", xlab = "Residuals")


# Complete Model
residuals_aic3 <- residuals(lm_aic3)
hist(residuals_aic3, breaks = 20, 
     main = "Residuals - Complete Model", xlab = "Residuals")

# Feature Engineered Model
residuals_aic4 <- residuals(lm_aic4)
hist(residuals_aic4, breaks = 20, 
     main = "Residuals - Feature Engineered Model", xlab = "Residuals")

# Resetting the layout to default
par(mfrow = c(1, 1))
```

\nopagebreak

### Conclusions on predictors importance

The model without **`fireins`** has performed quite poorly in comparison with the others, confirming the importance key importance of this predictor.

The model with only **`fireins`** and **`area`** further confirms the importance of
this predictors.

The best performing model (AIC with featured engineering), which
obviously includes **`fireins`** had explained almost 99% of the
variability (R\^2), its residuals distribution was not perfectly
normal but a little right-skewed.

The featured engineering has helped and shown how the animal feature
was not really helpful for the prediction.

The combination of the correlated features might have reduced very little noise, contributing to a slightly lower RMSE and higher R-Squared. 

## 5. Finding the best model to predict rent prices

### Testing penalized approaches

The first method that we will use to predict the rents amount
is **Elastic net**. *Lasso* and *Ridge regression* won't be that useful
(we tried them), as they usually don't handle well multicollinearity (In
the case of lasso, it just picks one of the correlated and disregard the
other basically). On the contrary *Elastic Net* can balance well the L1 and L2 penalties by adjusting alpha.

```{r, include = FALSE}
set.seed(123) 
# Creating a dataframe to store model performance indicators
net_performance_df <- data.frame(Model = character(), RMSE = numeric(), 
                             R2 = numeric(), stringsAsFactors = FALSE)

# We encode cathegorical variables because Elastic Net cannot have factors
train_city_encoded <- model.matrix(~ city - 1, data = train_data)
test_city_encoded <- model.matrix(~ city - 1, data = test_data)

train_animal_encoded <- model.matrix(~ animal - 1, data = train_data)
test_animal_encoded <- model.matrix(~ animal - 1, data = test_data)

train_furniture_encoded <- model.matrix(~ furniture - 1, data = train_data)
test_furniture_encoded <- model.matrix(~ furniture - 1, data = test_data)

# Encoding variables with the remaining predictors
train_xnet <- cbind(train_data[, -which(names(train_data) %in% 
                                        c("rent","city","animal","furniture"))], 
                    train_city_encoded, train_animal_encoded, 
                    train_furniture_encoded)
test_xnet <- cbind(test_data[, -which(names(test_data) %in% 
                                        c("rent","city","animal","furniture"))], 
                    test_city_encoded, test_animal_encoded, 
                    test_furniture_encoded)

# Scaling numerical variables
numeric_vars1 <- c("area","rooms","bathroom","parking.spaces","floor",
                   "hoa","proptax","fireins")

# Scale the numeric variables
scaled_train_x2net <- train_xnet
scaled_train_x2net[, numeric_vars1] <- scale(train_xnet[, numeric_vars1])
scaled_test_x2net <- test_xnet
scaled_test_x2net[, numeric_vars1] <- scale(test_xnet[, numeric_vars1])

# Convert the data frames to matrix format
train_xnet <- as.matrix(scaled_train_x2net)
test_xnet <- as.matrix(scaled_test_x2net)
```

```{r, echo = FALSE}
set.seed(123) 
# Tuning lambda with cv 
# (we don't prioritize L1 or L2, mixed approach is best hence alpha = 0.5)
enet_model <- cv.glmnet(train_xnet, train_y, alpha = 0.5, nfolds = 10)

# Optimal lambda value (we are less conservative and we use min)
lambda_min <- enet_model$lambda.min

# Fitting the model
enet_model_fit <- glmnet(train_xnet, train_y, alpha = 0.5, lambda = lambda_min)

# Making the predictions
predictions_enet <- predict(enet_model_fit, newx = test_xnet)

# Appending model performance
r2_enet <- calculate_R2(test_y, predictions_enet)
rmse_enet <- sqrt(mean((predictions_enet - test_y)^2))
performance_df <- rbind(performance_df, c("Elastic Net", rmse_enet, r2_enet))
net_performance_df <- rbind(net_performance_df, 
                            c("Elastic Net", rmse_enet, r2_enet))
colnames(net_performance_df) <- c("Model", "RMSE", "R2")
net_performance_df
```

### Testing non-linear approaches

The first method that we tried is **GAM splines** which are good for this situation
since through splines it should detect the non-linear relationships that the 
linear models didn't manage to find. Splines are special functions that can 
detect non-linear relationships and introduce smooth curves that fit the data.

```{r, echo = FALSE}
set.seed(123) 
gam_performance_df <- data.frame(Model = character(), RMSE = numeric(), 
                                 R2 = numeric(), stringsAsFactors = FALSE)

# Splines functions applied to variables less correlated with target (likely non-linear relationships)
gam.spline <- gam(rent ~ s(hoa) + s(proptax) + area + rooms + city + 
                      bathroom + floor + parking.spaces + fireins + 
                      furniture, data = train_data)

# Performance GAM complete
predictionsgam <- predict(gam.spline, newdata = test_x)
rmserfgam <- sqrt(mean((predictionsgam - test_y)^2))
r_squaredgam <- cor(predictionsgam, test_y)^2

gam_performance_df<-rbind(gam_performance_df,c("Gam", rmserfgam, r_squaredgam))

# Second model aims at mitigating correlation among predictors
gam.spline1 <- gam(rent ~ s(hoa * proptax) + s(area * rooms) + city + 
                     bathroom + floor + parking.spaces + s(fireins) + 
                     furniture, data = train_data)

# Performance GAM complete mitigated
predictionsgam1 <- predict(gam.spline1, newdata = test_x)
rmserfgam1 <- sqrt(mean((predictionsgam1 - test_y)^2))
r_squaredgam1 <- cor(predictionsgam1, test_y)^2
performance_df <- rbind(performance_df, 
                        c("Gam Spline", rmserfgam1, r_squaredgam1))
gam_performance_df <- rbind(gam_performance_df,c("Gam with FE", 
                                          rmserfgam1, r_squaredgam1))
colnames(gam_performance_df) <- c("Model", "RMSE", "R2")
gam_performance_df
```
### Our best Model : Random Forest

**Random Forest** solves most of the issues of other decision trees
methods. By bagging, it trains the decision trees with random
sampled data, avoiding correlation in predictions to
increase variability. As we did a gridsearch, the algorithm
showed very similar RMSE with different tuning parameters, proving
its robustness. Eventually we have chosen through **hyperparameter tuning** that the best one to have 500 trees and mtry set to 8 to reduce the risk of overfitting.

The more mtry, the more number of random
predictors in each tree, increasing the complexity of the model to understand more non-linear patterns between the predictors. 

```{r, echo = FALSE}
set.seed(123) 
# Setting up a dataframe for the metrics
RF_performance_df <- data.frame(Model = character(), RMSE = numeric(), 
                             R2 = numeric(), stringsAsFactors = FALSE)

set.seed(123) # To ensure reproducibility

# increasing mtry to equal all the predictor variables is bagging!
randomForest <- randomForest(x = train_x, y = train_y,
                                   ntrees = 500,
                                   mtry = 4);

# Predictions and measures
predictionsrfbag <- predict(randomForest, newdata = test_x)
rmserfbag <- sqrt(mean((predictionsrfbag - test_data$rent)^2))
r_squaredrf <- cor(predictionsrfbag, test_data$rent)^2
RF_performance_df <- rbind(RF_performance_df, c("Random forest Complete", 
                                          rmserfbag, r_squaredrf))


# Random forest without fireins
predictors_nofireins <- setdiff(names(train_x),c("fireins"))
train_x_nofireins <- train_x[,predictors_nofireins]

randomForest_nofireins <- randomForest(x = train_x_nofireins, y = train_y,
                                    ntrees = 500,
                                    mtry = 4);

# Predictions
predictionsrfbag_nofireins <- predict(randomForest_nofireins, newdata = test_x)
rmserfbag_nofireins <- sqrt(mean((predictionsrfbag_nofireins-test_data$rent)^2))
r_squaredrf_nofireins <- cor(predictionsrfbag_nofireins, test_data$rent)^2
RF_performance_df <- rbind(RF_performance_df, c("Random forest no fireins", 
                                          rmserfbag_nofireins, 
                                          r_squaredrf_nofireins))


# Data with feature engineering (multiplying closely related variables)
FE_train_data <- train_data
FE_test_data <- test_data
FE_train_data$area_rooms <- FE_train_data$area * FE_train_data$rooms
FE_train_data$hoa_proptax <- FE_train_data$hoa * FE_train_data$proptax
FE_test_data$area_rooms <- FE_test_data$area * FE_test_data$rooms
FE_test_data$hoa_proptax <- FE_test_data$hoa * FE_test_data$proptax

# Predictors
FE_predictors <- setdiff(names(FE_train_data),
                         c("rent","area","hoa","rooms","proptax"))
FE_train_x <- FE_train_data[,predictors]
FE_train_y <- FE_train_data$rent
FE_test_x <- FE_test_data[,predictors]
FE_test_y <- FE_test_data$rent

# Random Forest with Hyperparameter Tuning
HT_randomForest <- randomForest(x = FE_train_x, FE_train_y,
                             ntrees = 500,
                             mtry = 8);

# Predictions and measures
HT_predictionsrfbag <- predict(HT_randomForest, newdata = FE_test_x)
HT_rmserfbag <- sqrt(mean((HT_predictionsrfbag - test_data$rent)^2))
HT_r_squaredrf <- cor(HT_predictionsrfbag, test_data$rent)^2
RF_performance_df <- rbind(RF_performance_df, c("Random forest with FE & HT", 
                                                HT_rmserfbag, HT_r_squaredrf))
performance_df <- rbind(performance_df, c("Random forest", 
                                             HT_rmserfbag, HT_r_squaredrf))

# Returning models' performances
colnames(RF_performance_df) <- c("Model", "RMSE", "R2")
RF_performance_df <- RF_performance_df %>% arrange(RMSE)
RF_performance_df
RF_performance_df$RMSE <- as.numeric(RF_performance_df$RMSE)
RF_performance_df$R2 <- as.numeric(RF_performance_df$R2)
```

\nopagebreak

Our final results for the validation test of each model:
```{r, echo = FALSE}
colnames(performance_df) <- c("Model", "RMSE", "R2")
performance_df
```

## 6. Conclusions on the rent prediction models

On the one hand, the ability to adapt to different kind of houses is
crucial for the real estate agency and also for potential families which
want to invest money wisely. On the other, our results must be as
accurate as possible. We chose to use a "personalized" bootstrapping to
assess the model performance. We are basically taking 10 sampled
training and test sets and allowing replacement, unlike cross-validation
which does not train and test on already seen data (folds), we are
basically sampling 80% of the data for training and 20 as validation set
for 10 iterations. To enhance computational efficiency and capture
enough variability, we chose to sample 30% of the dataset and then split
80-20; Not only is the size that provides most reliable and stable
estimates (we tried sampling 50%, 63% and 80%) but also perfectly
depicts the concept of the **bias-variance tradeoff**.

```{r, echo = FALSE, fig.height= 5, fig.width= 10}
#lists to store performance results
plot1 <- list()
plot2 <- list()
plot3 <- list()

# Bootstrapping for each model and compute RMSE and R^2
num_iterations <- 10  
set.seed(123)  
for (i in 1:num_iterations) {
  
  # Split data into train and test sets (taking 30% sample data and splitting 80/20)
  train_datasample <- final_data[sample(nrow(final_data), 
                                        size = round(0.3 * nrow(final_data))), ]
  train_indices <- sample(nrow(train_datasample), 
                    size = floor(0.8 * nrow(train_datasample)), replace = FALSE)
  train_setcv <- train_datasample[train_indices, ]
  test_setcv <- train_datasample[-train_indices, ]
  
  traincv <- train_setcv[,-which(names(train_datasample) == "rent")]
  test_cv <- test_setcv[,-which(names(train_datasample) == "rent")]
  testy_cv <- test_setcv$rent
  
  # Random Forest with bagging
  randomForest.bagcv <- randomForest(x = traincv, y = train_setcv$rent,
                                       ntrees = 500,
                                       mtry = 11)
  
  predictionsrfbagcv <- predict(randomForest.bagcv, newdata = test_cv)
  rmserfbagcv <- 0
  r_squaredrfcv <- 0
  rmserfbagcv <- sqrt(mean((predictionsrfbagcv - testy_cv)^2))
  r_squaredrfcv <- cor(predictionsrfbagcv, testy_cv)^2
  plot1[i] <- rmserfbagcv
  
  #Best AIC
  lm_aic_cv <- stepAIC(lm(rent ~ hoa*proptax + area*rooms + city + bathroom +
                          parking.spaces + fireins + furniture, 
                          data = train_setcv), direction ="both", trace = FALSE)
  predictions_aic_cv <- predict(lm_aic_cv, newdata = test_cv)
  rmse_aic_cv <- 0
  R2_aic_cv <- 0
  rmse_aic_cv <- sqrt(mean((predictions_aic_cv - testy_cv)^2))
  R2_aic_cv <- calculate_R2(testy_cv, predictions_aic_cv)
  plot2[i] <- rmse_aic_cv
  
  # GAM with interaction terms
  m.gam.spline1cv <- gam(rent ~ s(hoa * proptax) + s(area * rooms) 
                         + city + bathroom + floor + parking.spaces + s(fireins) 
                         + furniture, data = train_setcv)
  predictionsgam1 <- predict(m.gam.spline1cv, newdata = test_cv)
  rmserfgam1cv <- 0
  r_squaredgam1cv <- 0
  rmserfgam1cv <- sqrt(mean((predictionsgam1 - testy_cv)^2))
  r_squaredgam1cv <- cor(predictionsgam1, testy_cv)^2
  plot3[i] <- rmserfgam1cv
  
}

# Plotting the variation of RMSE for each model
# Lists to vector to compute CI
plot1 <- unlist(plot1)
plot2 <- unlist(plot2)
plot3 <- unlist(plot3)

# Calculate mean and standard deviation of predictions for each model
mean_plot1 <- mean(plot1)
sd_plot1 <- sd(plot1)

mean_plot2 <- mean(plot2)
sd_plot2 <- sd(plot2)

mean_plot3 <- mean(plot3)
sd_plot3 <- sd(plot3)

# Calculate confidence intervals (95%)
ci_plot1 <- quantile(plot1, probs = c(0.025, 0.975))
ci_plot2 <- quantile(plot2, probs = c(0.025, 0.975))
ci_plot3 <- quantile(plot3, probs = c(0.025, 0.975))

# Plot mean predictions with confidence intervals
plot_data <- data.frame(
  Model = c("Random Forest", "AIC Linear Model", "GAM spline"),
  Mean = c(mean_plot1, mean_plot2, mean_plot3),
  Lower_CI = c(ci_plot1[1], ci_plot2[1], ci_plot3[1]),
  Upper_CI = c(ci_plot1[2], ci_plot2[2], ci_plot3[2])
)

# Plot
p <- ggplot(plot_data, aes(x = Model, y = Mean)) +
  geom_errorbar(aes(ymin = Lower_CI,ymax=Upper_CI),width = 0.2,color="blue") +
  geom_point(color = "blue") +
  xlab("Model") +
  ylab("Predictions") +
  ggtitle("Average RMSE with Confidence Intervals")

print(p)
```

The mean RMSE for the AIC Linear Model is the highest among the three models, and the confidence interval is also the widest, suggesting a higher degree of uncertainty in the model's performance compared to the other ones.

The GAM Spline Model shows a lower average RMSE than the AIC Linear Model. The confidence interval is relatively wide, indicating moderate uncertainty in predictions compared to the other ones.

The Random Forest model has the lowest mean RMSE, making it the most accurate predictor among the three. The confidence interval is narrower compared to the other models, showing higher confidence in its predictions. Furthermore, its narrower confidence interval shows a more consistent performance, suggesting higher reliability and robustness in predictions compared to the other two models.

\nopagebreak

## 7. Clustering

First and foremost the initial idea, other than identifing the rent value based on some features of the houses, was to get a comprehensive idea of the rental market and identify, for instance, the most convenient ones.

Clustering will help us identify diverse rental groups, giving advice, for instance, on where and how spacious an house should be to save some money. We will start by implementing **kmeans**, using the elbow method and silhouette score to assess the right number of clusters k.


```{r, echo = FALSE, fig.width = 7, fig.height = 2, fig.align = 'center'}
# Scaling the numerical variables data
data_scaled <- scale(final_data[,numeric_vars1])
data_scaled <- as.data.frame(data_scaled)

# Implementing the elbow method
k_values <- 1:15  # Range of k values to consider
withinss <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  k <- k_values[i]
  kmeans_result <- kmeans(data_scaled, centers = k)
  withinss[i] <- kmeans_result$tot.withinss
}

# Plot the elbow curve
elb <- ggplot() +
  geom_line(aes(x = k_values, y = withinss), color = "#FFA04A") +
  geom_point(aes(x = k_values, y = withinss), color = "#FFA04A") +
  labs(x = "Number of Clusters k", y = "Within-cluster Sum of Squares") +
  ggtitle("Elbow Method for Optimal k") +
  theme_bw() +
  theme(axis.line = element_line(colour = "black"), 
        axis.title = element_text(size = 8),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())

# Silhouette score by k and elbow printed
silk <- fviz_nbclust(data_scaled, kmeans, method='silhouette') +
        ggtitle("Silhouette Method for Optimal k")+
        theme(axis.title = element_text(size = 8))

# Plot the two plots in a single grid
grid.arrange(elb, silk, ncol = 2)
```

It is evident that both the **elbow method** and the **silhouette
scores** suggest that the right number of clusters is **k = 2**. Indeed the
silhouette score by k measures the quality of clustering, the higher it
is, the better, as it indicates that they are more distinguishable and
separate. On the other hand the within sum of squares indicated in the
elbow plot indicates roughly "how close" data points are in the cluster,
that is, how segregated they are (the higher, the better, and stronger
community). In the next steps we are going to perform also the clustering for
**k = 3**, logically it could be possible that, even if the scores are right and the best number of clusters should be two, in the context of the house market there are usually three cluster house types, so we will take this into account.

### k-Means

Now we will test with k-Means to see how the clusters adapt to the data:

```{r, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = 'center'}
# Plotting with 2 clusters
kmeans_model <- kmeans(data_scaled, centers = 2, nstart = 25)
clus2 <- fviz_cluster(kmeans_model, data = data_scaled, geom = "point",
                      main = paste("K-Means Clustering (k =", 2, ")"))

# Plotting with 3 clusters
kmeans_model2 <- kmeans(data_scaled, centers = 3, nstart = 25)
clus3 <- fviz_cluster(kmeans_model2, data = data_scaled, geom = "point",
                      main = paste("K-Means Clustering (k =", 3, ")"))

# Plotting the results
grid.arrange(clus2, clus3, ncol = 2)
```

The clustering with k = 3 revealed a more nuanced division in clusters that we will take into account.

\nopagebreak

### Hierarchical clustering

Now through Hierarchical clustering we want to effectively
compare which method works best to identify the distinct home groups. We
compared the euclidean distance and row correlation, but the first behaves better
overall, delineating clearer and more defined clusters. 

***Note that the
dendogram plot is not included due to report lenght restriction, in the section below we will describe what it represents.***

```{r, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = 'center'}
# By Euclidean distance works the best
dist_euc <- dist(data_scaled, method = "euclidean")
hc_euclidean <- hclust(dist_euc, method = "ward.D2")

# Assessing the results for different k
silhouette_euclidean <- rep(0, 2)
for (k in 2:3) {
  
  # Compute cluster assignments for euclidean distance
  hc_euclidean_k <- cutree(hc_euclidean, k = k)
  
  # Compute silhouette score for euclidean distance
  sileucscores <- data.frame(silhouette(hc_euclidean_k, dist_euc))
  mean_sil_width <- mean(as.numeric(sileucscores$sil_width))         
  silhouette_euclidean[k - 1] <-  mean_sil_width
  
}
cat("Silhouette scores for k = 2:", silhouette_euclidean[1], " and k = 3:", silhouette_euclidean[2],"\n")
```

For Hierarchical clustering k = 2 and k = 3 are again the best parameters,
with k = 2 the model finds 2 very distant clusters and, with k = 3,
two closer groups (presumably middle and low income houses) and a separate
group (more expensive houses) when we cut the three for 3 clusters.

```{r, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = 'center'}
# Compute cluster assignments for euclidean distance
hc_euclidean_k2 <- cutree(hc_euclidean, k = 2)
hc_euclidean_k3 <- cutree(hc_euclidean, k = 3)
  
# Plot clusters for euclidean distance
plot_title <- paste0("Hierarchical Clusters for k = 2")
plot_k2 <- fviz_cluster(list(data = data_scaled, 
                                       cluster = hc_euclidean_k2), 
                                  geom = "point", 
                                  palette = "jco", 
                                  main = plot_title) + theme_bw()

# Plot clusters for euclidean distance
plot_title <- paste0("Hierarchical Clusters for k = 3")
plot_k3 <- fviz_cluster(list(data = data_scaled, 
                                       cluster = hc_euclidean_k3), 
                                  geom = "point", 
                                  palette = "jco", 
                                  main = plot_title) + theme_bw()

# Plot the cluster assignment results for k = 2 and k = 3
grid.arrange(plot_k2, plot_k3, ncol = 2)
```

Having compared **Hierarchical** and **Kmeans** methods, the results are
pretty clear.**Kmeans** behaves
better as clusters are more separate and segregated (both visually and
in terms of silhouettes scores).

## 8. Conclusions on the Clustering process

To answer our initial question (due to the fact that we don't have a test set for our clusters) 
we decided to use k = 3 as the most insightful number of clusters, because in
general houses are divided into 3 categories, small/low income ones, mid
income, and luxury ones. It is important to understand that silhouette
score and the elbow method are not always enough to choose the correct k,
because the context and the analysis objective come first.
In fact integrating common knowledge with the given metrics used in our opinion is the way to go for our goal, that is to find clusters that can be insightful to undestrand the house rental market. k = 3 was the second best silhouette score and the most common cluster number for the house market class division.

With the following visualization we can see the distribution of the clusters based on some of the most important variables:

\nopagebreak

```{r, echo = FALSE, warning = FALSE, fig.width = 8, fig.height = 7, fig.align = 'center'}
data_new <- final_data

# 3 clusters
kmeans_result3 <- kmeans(data_scaled, centers = 3, nstart = 25)
data_new$clusters3 <- kmeans_result3$cluster

# 2 clusters
kmeans_result2 <- kmeans(data_scaled, centers = 2, nstart = 25)
data_new$clusters2 <- kmeans_result2$cluster

# Create the boxplot for city with regrouped 3 clusters

# By rent
box1 <- ggplot(data_new, aes(x = interaction(clusters3, city), 
                             y = rent, fill = as.factor(clusters3))) +
  geom_boxplot(color = "black") +
  labs(x = "clusters", y = "Rent") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        plot.title = element_text(size = 12)) +
  ggtitle("Rental prices by city and cluster") +
  theme(legend.position = "none")

# By area
box2 <- ggplot(data_new, aes(x = interaction(clusters3, city), 
                             y = area, fill = as.factor(clusters3))) +
  geom_boxplot(color = "black") +
  labs(x = "clusters", y = "area") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        plot.title = element_text(size = 12)) +
  ggtitle("Houses' area by cluster and city") +
  theme(legend.position = "none")

# By furniture
box3 <- ggplot(data_new, aes(x = interaction(clusters3, furniture), 
                             y = rent, fill = as.factor(clusters3))) +
  geom_boxplot(color = "black") +
  labs(x = "clusters", y = "rent") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        plot.title = element_text(size = 12)) +
  ggtitle("Houses' rent by cluster (furnished or not)") +
  theme(legend.position = "none")

box4 <- ggplot(data_new, aes(x = interaction(clusters3, furniture), 
                             y = area, fill = as.factor(clusters3))) +
  geom_boxplot(color = "black") +
  labs(x = "clusters", y = "area") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        plot.title = element_text(size = 12)) +
  ggtitle("Houses' area by cluster (furnished or not)") +
  theme(legend.position = "none")

# Arranging plots
grid.arrange(box1, box2, box3, box4, ncol = 2)
```

### Final considerations on the Clustering

Looking at the 3 clusters boxplots, the results are even clearer than
before. For buyers interested in renting here there are some example of information 
that they can get from our clustering data:

-   Kmeans has differentiated houses mainly by area and rent price. Although
    some outliers are visible in the clusters, it is easy to see that
    the most expensive houses (given same area) are those in Sao Paulo
    and Rio de Janeiro. Furthermore in this cities small houses belonging to           cluster 1 in particular are very overpriced when compared to other                 cities, confirm that this two cities have rents that cost more in general. 
    
-   It is worth buying furnished houses because their area is
    comparable to not furnished ones and the difference in rental prices
    is low. 

-   Houses in Campinas are much more worth the money. Although they may
    not be as appealing as the more luxurious and fancier in Sao Paulo,
    the cluster 1 and 2 revealed that mid class houses in Campinas are on
    average bigger and cheaper. 










